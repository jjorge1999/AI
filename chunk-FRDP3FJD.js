import{a as S}from"./chunk-4HTPA3GT.js";import{K as l,P as f,U as A,i as s,j as n,o as m,pc as d,x as a}from"./chunk-N535F6OD.js";import{g}from"./chunk-7QOE2ANW.js";var M=(()=>{class c{constructor(e){this.settingsService=e,this.sentimentPipeline=null,this.generationPipeline=null,this.isLoading=!1,this.localModelsSupported=!0,this.HF_API_URL="https://router.huggingface.co/v1/chat/completions",this.useGemmaApi=!0,this.AI_MODELS=["google/gemma-2-2b-it","meta-llama/Llama-3.2-3B-Instruct","mistralai/Mistral-7B-Instruct-v0.3","Qwen/Qwen2.5-1.5B-Instruct"],this.currentModelIndex=0}getHfToken(){return this.settingsService.getHuggingFaceToken()||d.huggingFaceToken||null}setHuggingFaceToken(e){localStorage.setItem("hf_token",e),console.log("AI Service: Hugging Face token set")}isGemmaApiAvailable(){let e=this.getHfToken();return!!e&&e.startsWith("hf_")}setUseGemmaApi(e=!0){this.useGemmaApi=e,console.log(`AI Service: Using ${e?"Gemma API":"Local Models"}`)}init(){return this.sentimentPipeline&&this.generationPipeline||this.isLoading?n(void 0):this.localModelsSupported?(this.isLoading=!0,s(import("./chunk-C33ARDKD.js")).pipe(l(e=>g(this,null,function*(){let{pipeline:t}=e;this.sentimentPipeline||(this.sentimentPipeline=yield t("sentiment-analysis","Xenova/distilbert-base-uncased-finetuned-sst-2-english")),this.generationPipeline||(this.generationPipeline=yield t("text2text-generation","Xenova/LaMini-Flan-T5-77M"))})),m(()=>{console.log("AI Service: Local models loaded successfully"),this.isLoading=!1}),a(e=>(console.error("AI Service: Failed to load local AI models",e),console.log("AI Service: Falling back to Gemma API only"),this.localModelsSupported=!1,this.isLoading=!1,n(void 0))))):(console.log("AI Service: Local models not supported on this browser, using Gemma API only"),n(void 0))}analyzeSentiment(e){return this.init().pipe(l(()=>this.sentimentPipeline?s(this.sentimentPipeline(e)):n(null)),m(t=>Array.isArray(t)&&t.length>0?t[0]:null),a(t=>(console.error("AI Service: Sentiment analysis failed",t),n(null))))}generateText(e){return this.useGemmaApi&&this.getHfToken()?this.generateWithGemma(e).pipe(l(t=>t?n(t):(console.log("AI Service: Gemma API failed, falling back to local model"),this.generateWithLocalModel(e)))):this.generateWithLocalModel(e)}generateWithGemma(e){let t=this.getHfToken();return t?this.tryModelWithFallback(e,t,0):(console.warn("AI Service: No Hugging Face token available for Gemma API"),n(null))}tryModelWithFallback(e,t,o){if(o>=this.AI_MODELS.length)return console.warn("AI Service: All models exhausted, falling back to local"),n(null);let r=this.AI_MODELS[o];return console.log(`AI Service: Trying model ${r}`),s(fetch(this.HF_API_URL,{method:"POST",headers:{Authorization:`Bearer ${t}`,"Content-Type":"application/json"},body:JSON.stringify({model:r,messages:[{role:"user",content:e}],max_tokens:150,temperature:.7})})).pipe(l(i=>g(this,null,function*(){if(!i.ok){let u=yield i.json().catch(()=>({}));return console.error(`AI Service: Model ${r} error`,i.status,u),i.status===402||i.status===429?(console.log(`AI Service: ${r} rate limited, trying next model...`),{tryNext:!0}):i.status===503?(console.log(`AI Service: ${r} is loading, trying next model...`),{tryNext:!0}):null}return i.json()})),l(i=>{if(i&&i.tryNext)return this.tryModelWithFallback(e,t,o+1);if(!i)return n(null);if(i.choices&&i.choices.length>0){let h=i.choices[0].message?.content?.trim()||null;return h&&(console.log(`AI Service: Successfully used model ${r}`),this.currentModelIndex=o),n(h)}return n(null)}),a(i=>(console.error(`AI Service: ${r} failed`,i),o+1<this.AI_MODELS.length?this.tryModelWithFallback(e,t,o+1):n(null))))}generateWithLocalModel(e){return this.init().pipe(l(()=>this.generationPipeline?s(this.generationPipeline(e,{max_new_tokens:60,temperature:.7,repetition_penalty:1.2})):n(null)),m(t=>Array.isArray(t)&&t.length>0?t[0].generated_text:null),a(t=>(console.error("AI Service: Local generation failed",t),n(null))))}chatWithGemma(e){let t=this.getHfToken();if(!t)return console.warn("AI Service: No Hugging Face token for Gemma chat"),n(null);let o=e.map(r=>({role:r.role==="model"?"assistant":r.role,content:r.content}));return this.tryChatWithFallback(o,t,0)}tryChatWithFallback(e,t,o){if(o>=this.AI_MODELS.length)return console.warn("AI Service: All models exhausted for chat"),n(null);let r=this.AI_MODELS[o];return console.log(`AI Service: Trying chat model ${r}`),s(fetch(this.HF_API_URL,{method:"POST",headers:{Authorization:`Bearer ${t}`,"Content-Type":"application/json"},body:JSON.stringify({model:r,messages:e,max_tokens:200,temperature:.7})})).pipe(l(i=>g(this,null,function*(){if(!i.ok){let u=yield i.json().catch(()=>({}));return console.error(`AI Service: Chat model ${r} error`,i.status,u),i.status===402||i.status===429||i.status===503?(console.log(`AI Service: ${r} unavailable, trying next...`),{tryNext:!0}):null}return i.json()})),l(i=>{if(i&&i.tryNext)return this.tryChatWithFallback(e,t,o+1);if(!i)return n(null);if(i.choices&&i.choices.length>0){let h=i.choices[0].message?.content?.trim()||null;return h&&console.log(`AI Service: Chat successfully used model ${r}`),n(h)}return n(null)}),a(i=>(console.error(`AI Service: Chat ${r} failed`,i),o+1<this.AI_MODELS.length?this.tryChatWithFallback(e,t,o+1):n(null))))}static{this.\u0275fac=function(t){return new(t||c)(A(S))}}static{this.\u0275prov=f({token:c,factory:c.\u0275fac,providedIn:"root"})}}return c})();export{M as a};
